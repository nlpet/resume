<!doctype html>
<html lang="en">
<head>
    <meta charset="UTF-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Nora Petrova. AI Research Engineer</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+3:ital,wght@0,300..900;1,300..900&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/css/all.min.css">
    <link rel="stylesheet" href="styles.css">
    <base target="_blank">
</head>
<body>
<div class="page">

    <header class="header">
        <h1>Nora Petrova</h1>
        <div class="header__title">AI Research Engineer</div>
        <div class="header__meta">
            <time datetime="">Last update: February 25, 2026</time>
            &nbsp;&middot;&nbsp;
            <span class="screen"><a href="nora-petrova.ai-research-engineer.pdf"><i class="far fa-file-pdf"></i> Download PDF</a></span>
            <span class="print">Up-to-date version at <a href="https://nlpet.github.io/resume">https://nlpet.github.io/resume</a></span>
        </div>
    </header>

    <div class="body">

        <aside class="sidebar">

            <section class="sidebar__section">
                <h2>Contact</h2>
                <ul class="contact-list">
                    <li><a href="https://www.google.com/maps/place/London,+UK/@51.5285582,-0.2416814,11z/data=!3m1!4b1!4m5!3m4!1s0x47d8a00baf21de75:0x52963a5addd52a99!8m2!3d51.5072178!4d-0.1275862"><i class="fa fa-home fact-icon"></i>London</a></li>
                    <li><a href="https://github.com/nlpet">
                  <i class="fa-brands fa-github fact-icon"></i>github.com/nlpet
                </a></li>
                    <li><a href="https://linkedin.com/in/nora-petrova">
              <i class="fa-brands fa-linkedin fact-icon"></i>linkedin.com/in/nora-petrova
            </a></li>
                    <li><a href="mailto:nora.axion@gmail.com"><i class="fa fa-envelope fact-icon"></i>nora.axion@gmail.com</a></li>
                </ul>
            </section>

            <section class="sidebar__section">
                <h2>Education</h2>
                <div class="education-entry">
                    <strong>BSc Mathematics &amp; Physics (hons)</strong>
                    <span class="edu-detail">&mdash; Open University, 2013&ndash;2018</span>
                </div>
                <div class="education-entry">
                    <strong>BA Informatics</strong>
                    <span class="edu-detail">&mdash; Molde University, 2007&ndash;2010</span>
                </div>
            </section>

            <section class="sidebar__section">
                <h2>Research Interests</h2>
                <div class="sidebar-entry">AI Safety &amp; Alignment</div>
                <div class="sidebar-entry">Mechanistic Interpretability</div>
                <div class="sidebar-entry">Adversarial Robustness &amp; Red Teaming</div>
                <div class="sidebar-entry">Behavioural, HITL &amp; Agentic Evals</div>
                <div class="sidebar-entry">Societal Impacts of AI</div>
            </section>

            <section class="sidebar__section">
                <h2>Skills</h2>
                <div class="sidebar-entry">Python, PyTorch, Julia, JS/TS</div>
                <div class="sidebar-entry">React, Full Stack, AWS, GCP</div>
                <div class="sidebar-entry">LLMs, NLP, Model Training &amp; Fine-tuning</div>
            </section>

            <section class="sidebar__section">
                <h2>Publications</h2>
                <div class="sidebar-entry">
                    <a href="https://arxiv.org/abs/2602.20813"><strong>Pressure Reveals Character</strong></a>
                    <span class="year">&mdash; arXiv, 2026</span>
                </div>
                <div class="sidebar-entry">
                    <a href="https://openreview.net/pdf?id=kVaE2kYjtV"><strong>Unpacking Human Preference for LLMs (HUMAINE)</strong></a>
                    <span class="year">&mdash; ICLR 2026</span>
                </div>
                <div class="sidebar-entry">
                    <a href="https://github.com/prolific-oss/commercial-pressure-evals"><strong>Commercial Pressure Evals</strong></a>
                    <span class="year">&mdash; IASEAI 2026</span>
                </div>
                <div class="sidebar-entry">
                    <a href="https://arxiv.org/abs/2504.18872"><strong>LAT Improves Representation of Refusal</strong></a>
                    <span class="year">&mdash; ICLR 2025</span>
                </div>
                <div class="sidebar-entry">
                    <a href="https://arxiv.org/abs/2409.15019"><strong>Evaluating Synthetic Activations from SAE Latents</strong></a>
                    <span class="year">&mdash; NeurIPS 2024</span>
                </div>
            </section>

            <section class="sidebar__section">
                <h2>Hackathons</h2>
                <div class="sidebar-entry">
                    <a href="https://apartresearch.com/project/who-does-your-ai-serve-manipulation-by-and-of-ai-assistants-77xx"><strong>AI Manipulation Hackathon</strong></a>
                    <span class="year">&mdash; Apart Research, 2026</span>
                </div>
                <div class="sidebar-entry">
                    <a href="https://github.com/nlpet/democracy-ai-hackathon"><strong>Democracy x AI</strong></a>
                    <span class="year">&mdash; Apart Lab, 2024</span>
                </div>
                <div class="sidebar-entry">
                    <a href="https://www.apartresearch.com/event/codered"><strong>Testing LLMs for Autonomous Capabilities</strong></a>
                    <span class="year">&mdash; Apart Lab &amp; METR, 2024</span>
                </div>
                <div class="sidebar-entry">
                    <a href="https://github.com/nlpet/anthropic-hackathon-claude"><strong>Anthropic Claude Hackathon</strong></a>
                    <span class="year">&mdash; Anthropic London, 2023</span>
                </div>
            </section>

        </aside>

        <main class="main">

            <section class="main__section">
                <h2>Summary</h2>
                <p class="summary">
                    AI Research Engineer building evaluation frameworks and interpretability tools to study
                    how frontier models align with human values. My work spans agentic, human-in-the-loop, and
                    behavioural evaluations of safety and alignment, mechanistic interpretability of model internals,
                    and large-scale studies of how people experience and collaborate with AI systems. I am driven by the
                    question of how we integrate AI into society responsibly, and I believe that rigorous evaluation,
                    mechanistic understanding, and empirical study of human-AI interaction are the foundations for
                    getting this right.
                </p>
            </section>

            <section class="main__section">
                <h2>Experience</h2>
                <div class="position">
                    <div class="position__header">Staff Research Engineer @ Prolific</div>
                    <div class="position__period">May 2023 - Present</div>
                    <div class="position__body"><ul><li>Leading AI evaluation research, building agentic and human-in-the-loop evaluation frameworks for studying safety and alignment in frontier models</li><li>Led <a href="https://huggingface.co/spaces/ProlificAI/humaine-leaderboard">HUMAINE</a>, a large-scale evaluation of human experience in using AI — paper accepted at <a href="https://openreview.net/pdf?id=kVaE2kYjtV">ICLR 2026</a></li><li>Developed <a href="https://github.com/prolific-oss/commercial-pressure-evals">commercial pressure evaluations</a> testing how frontier models respond when commercial objectives conflict with user safety, finding most models have no &quot;red line&quot; — won 1st place at the <a href="https://apartresearch.com/project/who-does-your-ai-serve-manipulation-by-and-of-ai-assistants-77xx">AI Manipulation Hackathon</a>, presenting at <a href="https://iaseai.org/">IASEAI 2026</a></li><li>Developed a behavioural alignment benchmark with an <a href="https://huggingface.co/spaces/nlpetprolific/alignment-leaderboard">interactive leaderboard</a> and co-authored <a href="https://arxiv.org/abs/2602.20813">Pressure Reveals Character</a></li><li>Working on agentic evaluations for ethical decision making, mechanistic interpretability of refusal circuits and black-box prefix attacks in open source models, and studying how to decouple reasoning from knowledge using tiny recursive models (TRMs)</li><li>Involved in cross-team projects including synthetic polling, agent detection, LLM usage detection, and multi-human-agent collaboration studies</li><li>Earlier work included developing an open source <a href="https://huggingface.co/datasets/ProlificAI/social-reasoning-rlhf">social reasoning RLHF dataset</a> and researching methods for aligning LLMs to human values</li></ul></div>
                </div>
                <div class="position">
                    <div class="position__header">AI Safety Researcher @ LASR Labs</div>
                    <div class="position__period">July 2024 - Sept 2024</div>
                    <div class="position__body"><p>Research into AI safety as part of the <a href="https://www.lasrlabs.org/">LASR Labs</a> 12-week research programme. Investigated how synthetic activations composed of SAE latents compare to real model activations in GPT-2, measuring sensitivity via KL divergence of output logits. Found that while synthetic activations behave comparably to real ones under sparsity and geometric similarity metrics, real activations contain structural properties beyond independent components. Paper <a href="https://arxiv.org/abs/2409.15019">Evaluating Synthetic Activations composed of SAE Latents in GPT-2</a> accepted at NeurIPS 2024.</p></div>
                </div>
                <div class="position">
                    <div class="position__header">AI Safety Researcher @ Apart Research</div>
                    <div class="position__period">May 2024 - Ongoing</div>
                    <div class="position__body"><p>Research fellowship (now ongoing collaboration) into AI safety alongside role at Prolific. Studied how <a href="https://arxiv.org/abs/2403.05030">latent adversarial training</a> (LAT) affects how language models encode refusal. Found that LAT concentrates refusal representation into fewer SVD components, making models more robust against external attacks but paradoxically more vulnerable to self-generated attack vectors. Paper <a href="https://arxiv.org/abs/2504.18872">LAT Improves the Representation of Refusal</a> accepted at ICLR 2025.</p></div>
                </div>
                <div class="position">
                    <div class="position__header">NLP Engineer / Julia Developer @ planting.space [contract]</div>
                    <div class="position__period">May 2023 - Mar 2024</div>
                    <div class="position__body"><ul><li>Developed a knowledge representation system by leveraging insights from Bayesian Inference, Category
  Theory and Natural Language Processing, using the Julia programming language and Python for NLP</li><li>Researched and implemented state of the art methods within NLP using LLMs, including advanced prompting
techniques and fine-tuning of models</li><li>Developed, deployed and maintained an internal search tool for finding relevant internal documents using
semantic search on chunked vectorised documents. The tool integrated with multiple internal systems</li></ul></div>
                </div>
                <div class="position">
                    <div class="position__header">Lead Software Engineer @ Signal.ai</div>
                    <div class="position__period">Apr 2022 - May 2023</div>
                    <div class="position__body"><ul><li>Led a team of engineers and researchers developing ML models and services for mention detection, entity disambiguation, sentiment analysis, text classification, and zero/few-shot learning</li><li>Researched state-of-the-art NLP techniques, prototyped applications, and developed production-ready pipeline services using transformer-based models</li><li>Built a RAG prototype searching against a large Elasticsearch cluster of news documents to respond to arbitrary queries</li></ul></div>
                </div>

                <h3 class="earlier-roles__heading">Earlier Roles</h3>
                <div class="earlier-roles">
                    <div class="earlier-role">
                        <span class="earlier-role__title">Senior Product Engineer</span>
                        <span class="earlier-role__company">Primer.ai</span>
                        <span class="earlier-role__period">2021 – 2022</span>
                    </div>
                    <div class="earlier-role">
                        <span class="earlier-role__title">ML Engineer → Principal ML Engineer</span>
                        <span class="earlier-role__company">Datatonic</span>
                        <span class="earlier-role__period">2018 – 2021</span>
                    </div>
                    <div class="earlier-role">
                        <span class="earlier-role__title">ML Engineer / Researcher</span>
                        <span class="earlier-role__company">Prodo.ai</span>
                        <span class="earlier-role__period">2017 – 2018</span>
                    </div>
                    <div class="earlier-role">
                        <span class="earlier-role__title">Full Stack Engineer</span>
                        <span class="earlier-role__company">R3PI, Digital Catapult, AXS</span>
                        <span class="earlier-role__period">2012 – 2017</span>
                    </div>
                </div>
            </section>

        </main>

    </div>

</div>
</body>
</html>
